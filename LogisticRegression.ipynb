{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wcrU5USj8Fx",
        "outputId": "8944ea06-3b9e-4dd1-f242-2f1e2dbf4e0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch= 10, loss= 0.578\n",
            "epoch= 20, loss= 0.474\n",
            "epoch= 30, loss= 0.408\n",
            "epoch= 40, loss= 0.362\n",
            "epoch= 50, loss= 0.329\n",
            "epoch= 60, loss= 0.303\n",
            "epoch= 70, loss= 0.283\n",
            "epoch= 80, loss= 0.266\n",
            "epoch= 90, loss= 0.252\n",
            "epoch= 100, loss= 0.240\n",
            "accuracy = 0.8860\n"
          ]
        }
      ],
      "source": [
        "#pytorch pipeline\n",
        "# 1) Design the model(input,output size, forward prop)\n",
        "# 2) loss and optimizer\n",
        "# 3) Train the model\n",
        "#  -> forward prop = prediction\n",
        "#  -> loss and gradient\n",
        "#  -> update weights\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler   #bcz we want to scale our features\n",
        "from sklearn.model_selection import train_test_split  # bcz we want to split our data for train and testing\n",
        "\n",
        "# 0) preprocessing of the data\n",
        "bc = datasets.load_breast_cancer() #breast cancer dataset is a binary classification problem\n",
        "X, y= bc.data, bc.target\n",
        "\n",
        "n_samples,n_features = X.shape\n",
        "#split our data\n",
        "X_train, x_test,y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=1234)\n",
        "#scale our features will make our features to have zero mean and unit variance. this is always needed when we deal with logistic regression\n",
        "\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "x_test = sc.fit_transform(x_test)\n",
        "\n",
        "#convert our data to torch tensor\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "x_test = torch.from_numpy(x_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0],1)\n",
        "y_test = y_test.view(y_test.shape[0],1)\n",
        "\n",
        "# 1) model\n",
        "# f= w*x + b, sigmoid at the end\n",
        "#we need to derive our own class\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features,1) #1=outputfeaturesno.\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "\n",
        "# 2) loss and optimizer\\\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()  #binary cross entropy loss\n",
        "optimizer = torch.optim.SGD(params = model.parameters(),lr = learning_rate)\n",
        "\n",
        "# 3) training loop\n",
        "iterations= 100\n",
        "for epoch in range(iterations):\n",
        "  #forwardprop = prediction\n",
        "  y_pred = model(X_train)\n",
        "\n",
        "  #loss\n",
        "  loss = criterion(y_pred,y_train)\n",
        "\n",
        "  #gradients\n",
        "  loss.backward()\n",
        "\n",
        "  #update the weights\n",
        "  optimizer.step()\n",
        "\n",
        "  #zero grad\n",
        "  optimizer.zero_grad()\n",
        "  \n",
        "  if((epoch+1)%10==0):\n",
        "    \n",
        "    print(f'epoch= {epoch+1}, loss= {loss:.3f}')\n",
        "    \n",
        "\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(x_test)\n",
        "  y_predicted_cls = y_predicted.round()  #if we had not written touch.nograd, then this statement would have changed the gradients in this y_predicted\n",
        "  #we need accuracy of this y_predicted_cls\n",
        "  accuracy = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'accuracy = {accuracy:.4f}')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "  "
      ],
      "metadata": {
        "id": "eAJmbnq9oH3f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}